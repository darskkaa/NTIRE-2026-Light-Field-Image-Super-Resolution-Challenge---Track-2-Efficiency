"""
Masked Angular Pre-Training Module for Light Field Super-Resolution
====================================================================

Reference: LFTransMamba (NTIRE 2025 2nd Place)
Paper: https://openaccess.thecvf.com/content/CVPR2025W/NTIRE/papers/Jin_LFTransMamba

This module implements masked angular pre-training for LFSR, which:
- Randomly masks angular views during training (train-time only)
- Forces the model to learn strong angular correlations
- Adds +0.2-0.3 dB PSNR without any inference cost

Usage:
    from utils.masked_pretraining import MaskedAngularPretraining
    
    mask_augment = MaskedAngularPretraining(
        angRes=5,
        mask_ratio=0.3,  # Mask 30% of views
        mask_strategy='random'  # or 'grid', 'corners'
    )
    
    # In training loop:
    lr_masked, mask_info = mask_augment(lr_data)
    out = model(lr_masked, data_info)
    loss = criterion(out, hr_data)  # HR is NOT masked
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import math
from typing import Tuple, Optional, List


class MaskedAngularPretraining(nn.Module):
    """
    Masked Angular Pre-Training for Light Field Super-Resolution.
    
    Masks random angular views in the LR input during training,
    forcing the network to learn robust angular correlations.
    
    Key insight: The HR target is NOT masked - the network must
    reconstruct complete angular information from partial input.
    
    Args:
        angRes: Angular resolution (e.g., 5 for 5x5 views)
        mask_ratio: Fraction of views to mask (0.0-0.5 recommended)
        mask_strategy: How to select views for masking
            - 'random': Random selection (default)
            - 'grid': Checkerboard pattern
            - 'corners': Mask corner views (preserve center)
            - 'center': Mask center views (preserve periphery)
        mask_value: Value to fill masked regions (0 or 'mean')
        enable_in_eval: If True, also mask during eval (for testing)
    
    Reference:
        LFTransMamba (CVPR 2025W): https://ieeexplore.ieee.org/document/11147739
        MaskLF (ECCV 2024): Masked pre-training for light fields
    """
    
    def __init__(
        self,
        angRes: int = 5,
        mask_ratio: float = 0.3,
        mask_strategy: str = 'random',
        mask_value: str = 'zero',
        enable_in_eval: bool = False
    ):
        super(MaskedAngularPretraining, self).__init__()
        
        self.angRes = angRes
        self.mask_ratio = mask_ratio
        self.mask_strategy = mask_strategy
        self.mask_value = mask_value
        self.enable_in_eval = enable_in_eval
        
        # Calculate number of views to mask
        self.total_views = angRes * angRes
        self.num_masked = max(1, int(self.total_views * mask_ratio))
        
        # Center view should never be masked (most important)
        self.center = (angRes // 2, angRes // 2)
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, dict]:
        """
        Apply masked angular pre-training.
        
        Args:
            x: Input LF in SAI format [B, C, angRes*H, angRes*W]
        
        Returns:
            x_masked: Masked input tensor
            mask_info: Dictionary with masking information
        """
        # Skip if in eval mode and not enabled
        if not self.training and not self.enable_in_eval:
            return x, {'masked': False, 'mask_ratio': 0.0}
        
        # Skip with probability (gradual curriculum)
        if random.random() > 0.5:  # 50% chance to skip masking
            return x, {'masked': False, 'mask_ratio': 0.0}
        
        B, C, H, W = x.shape
        h = H // self.angRes
        w = W // self.angRes
        
        # Get mask indices based on strategy
        mask_indices = self._get_mask_indices()
        
        # Apply masking
        x_masked = x.clone()
        
        for (i, j) in mask_indices:
            # Calculate view boundaries in SAI format
            y_start = i * h
            y_end = (i + 1) * h
            x_start = j * w
            x_end = (j + 1) * w
            
            # Apply mask value
            if self.mask_value == 'zero':
                x_masked[:, :, y_start:y_end, x_start:x_end] = 0
            elif self.mask_value == 'mean':
                mean_val = x[:, :, y_start:y_end, x_start:x_end].mean()
                x_masked[:, :, y_start:y_end, x_start:x_end] = mean_val
            elif self.mask_value == 'noise':
                x_masked[:, :, y_start:y_end, x_start:x_end] = torch.randn_like(
                    x[:, :, y_start:y_end, x_start:x_end]
                ) * 0.1
        
        mask_info = {
            'masked': True,
            'mask_ratio': len(mask_indices) / self.total_views,
            'mask_indices': mask_indices,
            'strategy': self.mask_strategy
        }
        
        return x_masked, mask_info
    
    def _get_mask_indices(self) -> List[Tuple[int, int]]:
        """Get indices of views to mask based on strategy."""
        
        all_views = [(i, j) for i in range(self.angRes) for j in range(self.angRes)]
        # Remove center view (never mask)
        all_views = [v for v in all_views if v != self.center]
        
        if self.mask_strategy == 'random':
            # Random selection
            return random.sample(all_views, min(self.num_masked, len(all_views)))
        
        elif self.mask_strategy == 'grid':
            # Checkerboard pattern (mask every other view)
            return [(i, j) for i, j in all_views if (i + j) % 2 == 0][:self.num_masked]
        
        elif self.mask_strategy == 'corners':
            # Mask corner views first
            corners = [(0, 0), (0, self.angRes-1), 
                      (self.angRes-1, 0), (self.angRes-1, self.angRes-1)]
            return [c for c in corners if c in all_views][:self.num_masked]
        
        elif self.mask_strategy == 'center':
            # Mask views near center (but not center itself)
            center_i, center_j = self.center
            distances = [(abs(i - center_i) + abs(j - center_j), (i, j)) 
                        for i, j in all_views]
            distances.sort(key=lambda x: x[0])
            return [v for _, v in distances[:self.num_masked]]
        
        else:
            return random.sample(all_views, min(self.num_masked, len(all_views)))


class ProgressiveMasking(nn.Module):
    """
    Progressive masking curriculum for training.
    
    Starts with low mask ratio and gradually increases during training.
    This helps the model first learn basic SR, then angular understanding.
    
    Args:
        angRes: Angular resolution
        start_ratio: Initial mask ratio (e.g., 0.1)
        end_ratio: Final mask ratio (e.g., 0.4)
        warmup_epochs: Epochs to reach end_ratio
    """
    
    def __init__(
        self,
        angRes: int = 5,
        start_ratio: float = 0.1,
        end_ratio: float = 0.4,
        warmup_epochs: int = 20
    ):
        super(ProgressiveMasking, self).__init__()
        
        self.angRes = angRes
        self.start_ratio = start_ratio
        self.end_ratio = end_ratio
        self.warmup_epochs = warmup_epochs
        self.current_epoch = 0
        
        self.masker = MaskedAngularPretraining(angRes=angRes, mask_ratio=start_ratio)
    
    def set_epoch(self, epoch: int):
        """Update current epoch and adjust mask ratio."""
        self.current_epoch = epoch
        
        # Linear interpolation of mask ratio
        progress = min(1.0, epoch / self.warmup_epochs)
        current_ratio = self.start_ratio + progress * (self.end_ratio - self.start_ratio)
        
        # Update masker
        self.masker.mask_ratio = current_ratio
        self.masker.num_masked = max(1, int(self.masker.total_views * current_ratio))
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, dict]:
        return self.masker(x)


def apply_masked_pretraining(
    data: torch.Tensor,
    angRes: int = 5,
    mask_ratio: float = 0.3,
    training: bool = True
) -> torch.Tensor:
    """
    Functional interface for masked pre-training.
    
    Args:
        data: Input LR light field [B, C, angRes*H, angRes*W]
        angRes: Angular resolution
        mask_ratio: Fraction of views to mask
        training: Whether in training mode
    
    Returns:
        Masked data (or original if not training)
    """
    if not training:
        return data
    
    # Random skip (curriculum)
    if random.random() > 0.5:
        return data
    
    B, C, H, W = data.shape
    h, w = H // angRes, W // angRes
    
    # Select random views to mask (excluding center)
    center = (angRes // 2, angRes // 2)
    all_views = [(i, j) for i in range(angRes) for j in range(angRes) if (i, j) != center]
    num_mask = max(1, int(len(all_views) * mask_ratio))
    mask_views = random.sample(all_views, num_mask)
    
    # Apply masking
    data_masked = data.clone()
    for (i, j) in mask_views:
        data_masked[:, :, i*h:(i+1)*h, j*w:(j+1)*w] = 0
    
    return data_masked


# ============================================================================
# SELF-TEST
# ============================================================================
if __name__ == '__main__':
    print("="*70)
    print("Masked Angular Pre-Training Module Test")
    print("="*70)
    
    # Test basic masking
    angRes = 5
    mask_aug = MaskedAngularPretraining(angRes=5, mask_ratio=0.3, mask_strategy='random')
    mask_aug.train()
    
    # Create test input (5x5 angular, 32x32 spatial)
    x = torch.randn(2, 1, 5*32, 5*32)
    
    print(f"\nInput shape: {x.shape}")
    print(f"Angular resolution: {angRes}x{angRes}")
    print(f"Mask ratio: 0.3 (30%)")
    
    # Apply masking
    x_masked, info = mask_aug(x)
    
    print(f"\nOutput shape: {x_masked.shape}")
    print(f"Masking applied: {info['masked']}")
    if info['masked']:
        print(f"Views masked: {len(info['mask_indices'])} / {angRes*angRes}")
        print(f"Mask indices: {info['mask_indices']}")
    
    # Test progressive masking
    print("\n" + "-"*70)
    print("Progressive Masking Test")
    print("-"*70)
    
    prog_mask = ProgressiveMasking(angRes=5, start_ratio=0.1, end_ratio=0.4, warmup_epochs=20)
    prog_mask.train()
    
    for epoch in [0, 5, 10, 15, 20]:
        prog_mask.set_epoch(epoch)
        _, info = prog_mask(x)
        print(f"Epoch {epoch:2d}: mask_ratio = {prog_mask.masker.mask_ratio:.2f}, "
              f"num_masked = {prog_mask.masker.num_masked}")
    
    # Test functional interface
    print("\n" + "-"*70)
    print("Functional Interface Test")
    print("-"*70)
    
    x_masked = apply_masked_pretraining(x, angRes=5, mask_ratio=0.3, training=True)
    print(f"Functional output shape: {x_masked.shape}")
    
    print("\nâœ… All tests passed!")
